Starting experiment: spikformer_d2_e256_m2_h16 on GPU 9
/home/e3da/.local/lib/python3.10/site-packages/cupy/_environment.py:596: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda12x

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
wandb: Currently logged in as: ancilottoalberto (ancilottoalberto-fbk) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
/usr/local/lib/python3.10/dist-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:
  Expected `list[str]` but got `tuple` - serialized value may not be as expected
  Expected `list[str]` but got `tuple` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
wandb: setting up run vxnt67bl
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /home/e3da/code/ehw_gesture/wandb/run-20251216_172727-vxnt67bl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spikformer_d2_e256_m2_h16
wandb: â­ï¸ View project at https://wandb.ai/ancilottoalberto-fbk/spikformer_ehwgesture-hyperparam-search
wandb: ðŸš€ View run at https://wandb.ai/ancilottoalberto-fbk/spikformer_ehwgesture-hyperparam-search/runs/vxnt67bl
Not using distributed mode
Namespace(model='spikformer', dataset='ehwgesture', num_classes=22, data_path='data/ehwgesture/', device='cuda:0', batch_size=16, workers=8, print_freq=256, output_dir='./logs/spikformer_d2_e256_m2_h16', resume='', sync_bn=False, test_only=False, amp=False, world_size=1, dist_url='env://', tb=True, T=16, opt='adamw', opt_eps=1e-08, opt_betas=None, weight_decay=0.06, momentum=0.9, connect_f='ADD', T_train=16, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, lr_cycle_mul=1.0, lr_cycle_limit=1, warmup_lr=1e-05, min_lr=1e-05, epochs=110, epoch_repeats=0.0, start_epoch=0, decay_epochs=20, warmup_epochs=10, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, smoothing=0.1, mixup=0.5, cutmix=0.0, cutmix_minmax=None, mixup_prob=0.5, mixup_switch_prob=0.5, mixup_mode='batch', mixup_off_epoch=0, log_wandb=True, wandb_project='spikformer_ehwgesture-hyperparam-search', wandb_entity=None, wandb_run_name='spikformer_d2_e256_m2_h16', patch_size=16, embed_dims=256, num_heads=16, mlp_ratios=2, in_channels=2, qkv_bias=False, depths=2, sr_ratios=1, drop_rate=0.0, drop_path_rate=0.1, drop_block_rate=None, train_ratio=0.75, random_split=False, distributed=False)
Loading data
The directory [data/ehwgesture/frames_number_16_split_by_number] already exists.
Took 342.21s
Training samples: 7289
Testing samples: 2419
Creating data loaders
===> Spikformer Block 0 MLP Granularity Info: 
{'granularities': [32, 96, 192, 512], 'num_granularities': 4, 'max_hidden': 512, 'params': [17248, 50208, 99648, 264448]}
===> Spikformer Block 0 Attention Granularity Info: 
{'head_granularities': [2, 4, 7, 16], 'num_granularities': 4, 'max_num_heads': 16, 'params': [28172, 54808, 94762, 214624]}
===> Spikformer Block 1 MLP Granularity Info: 
{'granularities': [32, 96, 192, 512], 'num_granularities': 4, 'max_hidden': 512, 'params': [17248, 50208, 99648, 264448]}
===> Spikformer Block 1 Attention Granularity Info: 
{'head_granularities': [2, 4, 7, 16], 'num_granularities': 4, 'max_num_heads': 16, 'params': [28172, 54808, 94762, 214624]}
Creating model
number of params: 2071830
purge_step_train=0, purge_step_te=0
Start training
Epoch: [0]  [  0/455]  eta: 2:08:45  lr: 1e-05  img/s: 1.9732427749232264  loss: 3.0986 (3.0986)  acc1: 0.0000 (0.0000)  acc5: 31.2500 (31.2500)  time: 16.9783  data: 8.8697  max mem: 11153
Epoch: [0]  [256/455]  eta: 0:03:01  lr: 1e-05  img/s: 22.459819970334195  loss: 3.0695 (3.0880)  acc1: 6.2500 (4.5720)  acc5: 25.0000 (24.1488)  time: 0.9158  data: 0.0059  max mem: 11803
Epoch: [0] Total time: 0:06:53
Test:  [  0/152]  eta: 0:24:35  loss: 3.0233 (3.0233)  acc1_g0: 0.0000 (0.0000)  acc5_g0: 100.0000 (100.0000)  time: 9.7052  data: 9.4015  max mem: 11803
Test:  [100/152]  eta: 0:00:31  loss: 3.1255 (3.0810)  acc1_g0: 0.0000 (6.9926)  acc5_g0: 0.0000 (26.6708)  time: 0.4505  data: 0.2493  max mem: 11803
Test: Total time: 0:01:27
Test:  [  0/152]  eta: 0:21:01  loss: 3.0340 (3.0640)  acc1_g0: 0.0000 (5.6635)  acc5_g0: 37.5000 (34.6011)  acc1_g1: 0.0000 (0.0000)  acc5_g1: 93.7500 (93.7500)  time: 8.2987  data: 8.0938  max mem: 11803
Test:  [100/152]  eta: 0:00:34  loss: 3.1239 (3.0671)  acc1_g0: 0.0000 (5.6635)  acc5_g0: 37.5000 (34.6011)  acc1_g1: 0.0000 (7.6114)  acc5_g1: 0.0000 (27.4752)  time: 0.5903  data: 0.3414  max mem: 11803
Test: Total time: 0:01:33
Test:  [  0/152]  eta: 0:27:35  loss: 3.0244 (3.0589)  acc1_g0: 0.0000 (5.6635)  acc5_g0: 37.5000 (34.6011)  acc1_g1: 0.0000 (8.0198)  acc5_g1: 37.5000 (32.8235)  acc1_g2: 0.0000 (0.0000)  acc5_g2: 68.7500 (68.7500)  time: 10.8882  data: 10.7807  max mem: 11803
Test:  [100/152]  eta: 0:00:33  loss: 3.1380 (3.0611)  acc1_g0: 0.0000 (5.6635)  acc5_g0: 37.5000 (34.6011)  acc1_g1: 0.0000 (8.0198)  acc5_g1: 37.5000 (32.8235)  acc1_g2: 0.0000 (7.3020)  acc5_g2: 0.0000 (23.7005)  time: 0.5803  data: 0.4226  max mem: 11803
Test: Total time: 0:01:30
wandb: WARNING Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.
Test:  [  0/152]  eta: 0:22:03  loss: 2.9993 (3.0532)  acc1_g0: 0.0000 (5.6635)  acc5_g0: 37.5000 (34.6011)  acc1_g1: 0.0000 (8.0198)  acc5_g1: 37.5000 (32.8235)  acc1_g2: 0.0000 (8.2265)  acc5_g2: 43.7500 (32.3687)  acc1_g3: 0.0000 (0.0000)  acc5_g3: 56.2500 (56.2500)  time: 8.7050  data: 8.4987  max mem: 11803
Test:  [100/152]  eta: 0:00:30  loss: 3.1738 (3.0550)  acc1_g0: 0.0000 (5.6635)  acc5_g0: 37.5000 (34.6011)  acc1_g1: 0.0000 (8.0198)  acc5_g1: 37.5000 (32.8235)  acc1_g2: 0.0000 (8.2265)  acc5_g2: 43.7500 (32.3687)  acc1_g3: 0.0000 (7.3020)  acc5_g3: 0.0000 (24.2574)  time: 0.6309  data: 0.4307  max mem: 11803
Test: Total time: 0:01:18
 * Acc@1 (G:0) = 5.663497312939231, Acc@1 (G:3) = 4.878048780487805, loss = 3.046950038326414
Namespace(model='spikformer', dataset='ehwgesture', num_classes=22, data_path='data/ehwgesture/', device='cuda:0', batch_size=16, workers=8, print_freq=256, output_dir='./logs/spikformer_d2_e256_m2_h16', resume='', sync_bn=False, test_only=False, amp=False, world_size=1, dist_url='env://', tb=True, T=16, opt='adamw', opt_eps=1e-08, opt_betas=None, weight_decay=0.06, momentum=0.9, connect_f='ADD', T_train=16, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, lr_cycle_mul=1.0, lr_cycle_limit=1, warmup_lr=1e-05, min_lr=1e-05, epochs=110, epoch_repeats=0.0, start_epoch=0, decay_epochs=20, warmup_epochs=10, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, smoothing=0.1, mixup=0.5, cutmix=0.0, cutmix_minmax=None, mixup_prob=0.5, mixup_switch_prob=0.5, mixup_mode='batch', mixup_off_epoch=0, log_wandb=True, wandb_project='spikformer_ehwgesture-hyperparam-search', wandb_entity=None, wandb_run_name='spikformer_d2_e256_m2_h16', patch_size=16, embed_dims=256, num_heads=16, mlp_ratios=2, in_channels=2, qkv_bias=False, depths=2, sr_ratios=1, drop_rate=0.0, drop_path_rate=0.1, drop_block_rate=None, train_ratio=0.75, random_split=False, distributed=False)
Training time 0:12:45 max_test_acc1 5.663497312939231 test_acc5_at_max_test_acc1 4.878048780487805
./logs/spikformer_d2_e256_m2_h16/spikformer_b16_T16_Ttrain16_wd0.06_adamw_cnf_ADD/lr0.001
Epoch: [1]  [  0/455]  eta: 0:55:13  lr: 0.00010899999999999999  img/s: 14.668382924935122  loss: 3.0866 (3.0866)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (12.5000)  time: 7.2825  data: 6.1917  max mem: 11803
Epoch: [1]  [256/455]  eta: 0:02:56  lr: 0.00010899999999999999  img/s: 19.990427309691544  loss: 2.8583 (2.9290)  acc1: 18.7500 (11.0165)  acc5: 50.0000 (42.5827)  time: 0.8000  data: 0.0051  max mem: 11803
Epoch: [1] Total time: 0:06:35
Test:  [  0/152]  eta: 0:21:32  loss: 2.9956 (2.9956)  acc1_g0: 0.0000 (0.0000)  acc5_g0: 68.7500 (68.7500)  time: 8.5005  data: 8.1977  max mem: 11803
Test:  [100/152]  eta: 0:00:34  loss: 2.9610 (2.9726)  acc1_g0: 0.0000 (11.1386)  acc5_g0: 43.7500 (51.0520)  time: 0.6053  data: 0.4012  max mem: 11803
Test: Total time: 0:01:36
Test:  [  0/152]  eta: 0:19:45  loss: 2.7331 (2.9381)  acc1_g0: 56.2500 (15.2129)  acc5_g0: 100.0000 (57.4618)  acc1_g1: 0.0000 (0.0000)  acc5_g1: 62.5000 (62.5000)  time: 7.8021  data: 7.6028  max mem: 11803
Test:  [100/152]  eta: 0:00:35  loss: 2.9226 (2.9070)  acc1_g0: 56.2500 (15.2129)  acc5_g0: 100.0000 (57.4618)  acc1_g1: 0.0000 (20.9158)  acc5_g1: 56.2500 (57.1163)  time: 0.5895  data: 0.3884  max mem: 11803
Test: Total time: 0:01:36
Test:  [  0/152]  eta: 0:29:22  loss: 2.3155 (2.8556)  acc1_g0: 56.2500 (15.2129)  acc5_g0: 100.0000 (57.4618)  acc1_g1: 66.6667 (22.3646)  acc5_g1: 100.0000 (62.0504)  acc1_g2: 0.0000 (0.0000)  acc5_g2: 62.5000 (62.5000)  time: 11.5926  data: 11.0933  max mem: 11803
Test:  [100/152]  eta: 0:00:34  loss: 2.9129 (2.8153)  acc1_g0: 56.2500 (15.2129)  acc5_g0: 100.0000 (57.4618)  acc1_g1: 66.6667 (22.3646)  acc5_g1: 100.0000 (62.0504)  acc1_g2: 0.0000 (22.2772)  acc5_g2: 37.5000 (57.3020)  time: 0.5396  data: 0.3043  max mem: 11803
Test: Total time: 0:01:35
Test:  [  0/152]  eta: 0:25:37  loss: 1.9618 (2.7649)  acc1_g0: 56.2500 (15.2129)  acc5_g0: 100.0000 (57.4618)  acc1_g1: 66.6667 (22.3646)  acc5_g1: 100.0000 (62.0504)  acc1_g2: 75.0000 (24.3902)  acc5_g2: 100.0000 (62.1331)  acc1_g3: 0.0000 (0.0000)  acc5_g3: 56.2500 (56.2500)  time: 10.1166  data: 9.9978  max mem: 11803
Test:  [100/152]  eta: 0:00:31  loss: 2.9336 (2.7442)  acc1_g0: 56.2500 (15.2129)  acc5_g0: 100.0000 (57.4618)  acc1_g1: 66.6667 (22.3646)  acc5_g1: 100.0000 (62.0504)  acc1_g2: 75.0000 (24.3902)  acc5_g2: 100.0000 (62.1331)  acc1_g3: 0.0000 (17.5124)  acc5_g3: 25.0000 (47.2153)  time: 0.3849  data: 0.2415  max mem: 11803
Test: Total time: 0:01:23
 * Acc@1 (G:0) = 15.212897888536835, Acc@1 (G:3) = 19.71889210417528, loss = 2.688774021636499
Namespace(model='spikformer', dataset='ehwgesture', num_classes=22, data_path='data/ehwgesture/', device='cuda:0', batch_size=16, workers=8, print_freq=256, output_dir='./logs/spikformer_d2_e256_m2_h16', resume='', sync_bn=False, test_only=False, amp=False, world_size=1, dist_url='env://', tb=True, T=16, opt='adamw', opt_eps=1e-08, opt_betas=None, weight_decay=0.06, momentum=0.9, connect_f='ADD', T_train=16, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, lr_cycle_mul=1.0, lr_cycle_limit=1, warmup_lr=1e-05, min_lr=1e-05, epochs=110, epoch_repeats=0.0, start_epoch=0, decay_epochs=20, warmup_epochs=10, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, smoothing=0.1, mixup=0.5, cutmix=0.0, cutmix_minmax=None, mixup_prob=0.5, mixup_switch_prob=0.5, mixup_mode='batch', mixup_off_epoch=0, log_wandb=True, wandb_project='spikformer_ehwgesture-hyperparam-search', wandb_entity=None, wandb_run_name='spikformer_d2_e256_m2_h16', patch_size=16, embed_dims=256, num_heads=16, mlp_ratios=2, in_channels=2, qkv_bias=False, depths=2, sr_ratios=1, drop_rate=0.0, drop_path_rate=0.1, drop_block_rate=None, train_ratio=0.75, random_split=False, distributed=False)
Training time 0:25:35 max_test_acc1 15.212897888536835 test_acc5_at_max_test_acc1 19.71889210417528
./logs/spikformer_d2_e256_m2_h16/spikformer_b16_T16_Ttrain16_wd0.06_adamw_cnf_ADD/lr0.001
9.5971  data: 8.6987  max mem: 12499
Epoch: [1]  [256/455]  eta: 0:03:01  lr: 0.00010899999999999999  img/s: 26.853309463320862  loss: 2.6787 (2.9040)  acc1: 18.7500 (13.8375)  acc5: 62.5000 (45.9144)  time: 0.7700  data: 0.0101  max mem: 12499
Epoch: [1] Total time: 0:06:30
Test:  [  0/152]  eta: 0:18:42  loss: 2.9741 (2.9741)  acc1_g0: 0.0000 (0.0000)  acc5_g0: 31.2500 (31.2500)  time: 7.3840  data: 7.1913  max mem: 12499
Test:  [100/152]  eta: 0:00:27  loss: 2.9631 (2.9818)  acc1_g0: 0.0000 (12.5619)  acc5_g0: 75.0000 (51.6089)  time: 0.6497  data: 0.4447  max mem: 12499
Test: Total time: 0:01:18
Test:  [  0/152]  eta: 0:28:37  loss: 2.8113 (2.9560)  acc1_g0: 43.7500 (16.6598)  acc5_g0: 100.0000 (58.5366)  acc1_g1: 0.0000 (0.0000)  acc5_g1: 68.7500 (68.7500)  time: 11.2994  data: 11.0939  max mem: 12499
Test:  [100/152]  eta: 0:00:34  loss: 2.9443 (2.9421)  acc1_g0: 43.7500 (16.6598)  acc5_g0: 100.0000 (58.5366)  acc1_g1: 0.0000 (12.5619)  acc5_g1: 75.0000 (52.7228)  time: 0.4808  data: 0.2695  max mem: 12499
Test: Total time: 0:01:38
Test:  [  0/152]  eta: 0:21:16  loss: 2.7208 (2.9178)  acc1_g0: 43.7500 (16.6598)  acc5_g0: 100.0000 (58.5366)  acc1_g1: 25.0000 (16.8665)  acc5_g1: 100.0000 (58.6193)  acc1_g2: 0.0000 (0.0000)  acc5_g2: 81.2500 (81.2500)  time: 8.3958  data: 7.8990  max mem: 12499
Test:  [100/152]  eta: 0:00:33  loss: 2.8950 (2.8777)  acc1_g0: 43.7500 (16.6598)  acc5_g0: 100.0000 (58.5366)  acc1_g1: 25.0000 (16.8665)  acc5_g1: 100.0000 (58.6193)  acc1_g2: 0.0000 (13.1807)  acc5_g2: 31.2500 (51.9802)  time: 0.5154  data: 0.3130  max mem: 12499
Test: Total time: 0:01:35
Test:  [  0/152]  eta: 0:25:05  loss: 2.0498 (2.8289)  acc1_g0: 43.7500 (16.6598)  acc5_g0: 100.0000 (58.5366)  acc1_g1: 25.0000 (16.8665)  acc5_g1: 100.0000 (58.6193)  acc1_g2: 25.0000 (17.8173)  acc5_g2: 100.0000 (57.2964)  acc1_g3: 0.0000 (0.0000)  acc5_g3: 56.2500 (56.2500)  time: 9.9062  data: 9.6133  max mem: 12499
Test:  [100/152]  eta: 0:00:28  loss: 2.6437 (2.8221)  acc1_g0: 43.7500 (16.6598)  acc5_g0: 100.0000 (58.5366)  acc1_g1: 25.0000 (16.8665)  acc5_g1: 100.0000 (58.6193)  acc1_g2: 25.0000 (17.8173)  acc5_g2: 100.0000 (57.2964)  acc1_g3: 0.0000 (13.4901)  acc5_g3: 25.0000 (44.0594)  time: 0.4758  data: 0.2739  max mem: 12499
Test: Total time: 0:01:22
 * Acc@1 (G:0) = 16.6597767672592, Acc@1 (G:3) = 18.313352625051674, loss = 2.7722468606539463
Namespace(model='spikformer', dataset='ehwgesture', num_classes=22, data_path='data/ehwgesture/', device='cuda:0', batch_size=16, workers=8, print_freq=256, output_dir='./logs/spikformer_d2_e256_m4_h16', resume='', sync_bn=False, test_only=False, amp=False, world_size=1, dist_url='env://', tb=True, T=16, opt='adamw', opt_eps=1e-08, opt_betas=None, weight_decay=0.06, momentum=0.9, connect_f='ADD', T_train=16, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, lr_cycle_mul=1.0, lr_cycle_limit=1, warmup_lr=1e-05, min_lr=1e-05, epochs=110, epoch_repeats=0.0, start_epoch=0, decay_epochs=20, warmup_epochs=10, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, smoothing=0.1, mixup=0.5, cutmix=0.0, cutmix_minmax=None, mixup_prob=0.5, mixup_switch_prob=0.5, mixup_mode='batch', mixup_off_epoch=0, log_wandb=True, wandb_project='spikformer_ehwgesture-hyperparam-search', wandb_entity=None, wandb_run_name='spikformer_d2_e256_m4_h16', patch_size=16, embed_dims=256, num_heads=16, mlp_ratios=4, in_channels=2, qkv_bias=False, depths=2, sr_ratios=1, drop_rate=0.0, drop_path_rate=0.1, drop_block_rate=None, train_ratio=0.75, random_split=False, distributed=False)
Training time 0:22:51 max_test_acc1 16.6597767672592 test_acc5_at_max_test_acc1 18.313352625051674
./logs/spikformer_d2_e256_m4_h16/spikformer_b16_T16_Ttrain16_wd0.06_adamw_cnf_ADD/lr0.001
Epoch: [2]  [  0/455]  eta: 1:08:58  lr: 0.000208  img/s: 14.679051145470963  loss: 2.7096 (2.7096)  acc1: 31.2500 (31.2500)  acc5: 68.7500 (68.7500)  time: 9.0958  data: 8.0058  max mem: 12499
Epoch: [2]  [256/455]  eta: 0:02:56  lr: 0.000208  img/s: 19.955759375440472  loss: 2.3317 (2.5142)  acc1: 25.0000 (25.8025)  acc5: 75.0000 (74.9270)  time: 0.9001  data: 0.0055  max mem: 12499
Epoch: [2] Total time: 0:06:32
